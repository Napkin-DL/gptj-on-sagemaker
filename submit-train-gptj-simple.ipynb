{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Tensor Parallelism using the SageMaker Model Parallelism Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the tensor parallelism feature provided by the SageMaker model parallelism library. You'll learn how to train the GPT-J model with tensor parallelism on a synthetic text data.\n",
    "\n",
    "**Note**: To run this example training job, you must be in `us-west-2`. The preview version of container images are available only in those two regions.\n",
    "\n",
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "#     !{sys.executable} -m pip install -U split-folders tqdm albumentations crc32c wget\n",
    "    !{sys.executable} -m pip install 'sagemaker[local]' --upgrade\n",
    "    !{sys.executable} -m pip install -U smdebug sagemaker-experiments\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !{sys.executable} -m pip install -U datasets transformers\n",
    "    !/bin/bash ./local/local_change_setting.sh\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "This private preview feature is available to use in `us-east-1` and `us-west-2`.\n",
    "Throughout this example, you'll use a training script of GPT model and a text dataset.\n",
    "\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "# If running in Sagemaker notebook this can stay commented\n",
    "# os.environ[\"AWS_PROFILE\"] = \"sm\"\n",
    "\n",
    "# supported regions only us-west-2 and us-east-1\n",
    "# preview images are only in these two regions\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Amazon S3 Bucket Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. As part of the private preview artifacts, we provide a synthetic dataset that you can use to quickly get started in 'smdistributed-modelparallel-preview' bucket. This bucket is in us-west-2, and we recommend you copy the data to your own bucket and update the paths in the next cell to avoid any cross-account permission issues depending on your IAM role permissions.\n",
    "\n",
    "After you successfully run this example tensor parallel training job, you can modify the S3 bucket to where your own dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dataset='s3://sagemaker-sample-files/datasets/binary/bert/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en_abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/wikicorpus_en_abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $external_dataset $data_path --recursive\n",
    "!rm -rf $data_path/train $data_path/test\n",
    "!mkdir $data_path/train $data_path/test\n",
    "!mv $data_path/*_training_* $data_path/train/\n",
    "!mv $data_path/*_test_* $data_path/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below bucket will store output artifacts of the training job. You can modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'dataset-us-west-2-cyj'  #<== 데이터셋이 들어있는 bucket 이름으로 변경\n",
    "input_data = f's3://{data_bucket}/wikicorpus_en_abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync ./dataset/wikicorpus_en_abstract $input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fsx and use fsx for data channels and checkpoints\n",
    "\n",
    "While the above option is easier to setup, using an FSX can be beneficial for performance when dealing with large input sizes and large model sizes. If you are using models above 13B, checkpointing should be done using FSX. \n",
    "\n",
    "Please see the instructions [here](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb), to create the FSx lustre filesystem and import the dataset from the S3 bucket to your fsx filesystem. Note that the FSX must be created in a private subnet with internet gateway to ensure that training job has access to the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions obtained from:\n",
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb\n",
    "\n",
    "use_fsx = False\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    # Specify FSx Lustre file system id.\n",
    "    file_system_id = \"<your-file-system-id>\"\n",
    "\n",
    "    # Specify the SG and subnet used by the FSX, these are passed to SM Estimator so jobs use this as well\n",
    "    fsx_security_group_id = \"<your-security-group-id>\"\n",
    "    fsx_subnet = \"<your-subnet>\"\n",
    "\n",
    "    # Specify directory path for input data on the file system.\n",
    "    # You need to provide normalized and absolute path below.\n",
    "    # Your mount name can be provided by you when creating fsx, or generated automatically.\n",
    "    # You can find this mount_name on the FSX page in console.\n",
    "    # Example of fsx generated mount_name: \"3x5lhbmv\"\n",
    "    base_path = \"<your-mount-name>\"\n",
    "\n",
    "    # Specify your file system type.\n",
    "    file_system_type = \"FSxLustre\"\n",
    "\n",
    "    fs_train = FileSystemInput(file_system_id=file_system_id,\n",
    "                            file_system_type=file_system_type,\n",
    "                            directory_path=train_base_path,\n",
    "                            file_system_access_mode=\"rw\")\n",
    "    fs_test = FileSystemInput(file_system_id=file_system_id,\n",
    "                            file_system_type=file_system_type,\n",
    "                            directory_path=test_base_path,\n",
    "                            file_system_access_mode=\"rw\")\n",
    "\n",
    "    data_channels = {\"train\": fs_train, \"test\": fs_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`train_gptj_simple.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'Batch', 'Regex': 'Batch:(.*?),'},\n",
    "     {'Name': 'train:Loss', 'Regex': 'Train loss:(.*?),'},\n",
    "     {'Name': 'train:speed', 'Regex': 'Train speed:(.*?),'},\n",
    "     {'Name': 'validation:Loss', 'Regex': 'Validation loss:(.*?),'},\n",
    "     {'Name': 'validation:perplexity', 'Regex': 'Validation perplexity:(.*?),'},    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'max_steps': 50,\n",
    "                   'seed': 12345,\n",
    "                   'fp16': 1,\n",
    "                   'lr': 2.e-4,\n",
    "                   'lr_decay_iters': 125000,\n",
    "                   'min_lr': 0.00001,\n",
    "                   'lr-decay-style': 'linear',\n",
    "                   'warmup': 0.01,\n",
    "                   'num_kept_checkpoints': 5,\n",
    "                   'checkpoint_freq': 200,\n",
    "                   'logging_freq': 10,\n",
    "                   'save_final_full_model': 1,\n",
    "                   'skip_full_optimizer': 1,\n",
    "                   'shard_optimizer_state': 1,\n",
    "                   'activation_checkpointing': 1,\n",
    "                   'activation_strategy': 'each',\n",
    "                   'optimize': 'speed',\n",
    "                   'use_bert_data': 1,\n",
    "                   'epochs': 20,\n",
    "                    # below flag loads model and optimizer state from checkpoint_s3_uri\n",
    "                    # 'load_partial': 1,\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "if input_data.split('/')[-1] != 'wikicorpus_en_abstract':\n",
    "    # those flags are used when training with the openwebtext dataset\n",
    "    hyperparameters[\"zipped_data\"] = 0\n",
    "    hyperparameters[\"validation_freq\"] = 20\n",
    "    hyperparameters[\"use_wiki_data\"] = 0\n",
    "    \n",
    "    \n",
    "# if use_fsx:\n",
    "#     # make sure to update paths for training-dir and test-dir based on the paths of datasets in fsx\n",
    "#     # If you want to resume training, set checkpoint-dir to the same path as a previous job.\n",
    "#     SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
    "#     hyperparameters['checkpoint-dir'] = f\"{SM_TRAIN_DIR}/checkpointdir-job2\"\n",
    "#     hyperparameters['model-dir'] = f\"{SM_TRAIN_DIR}/modeldir-job2\"\n",
    "#     hyperparameters['training-dir'] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/train_synthetic\"\n",
    "#     hyperparameters['test-dir'] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/val_synthetic\"\n",
    "\n",
    "# The checkpoint path (hyperparameters['checkpoint-dir'] or checkpoint_s3_uri) is not unique per job. \n",
    "# You need to modify as needed for different runs. \n",
    "# If same path is used for unrelated runs, this may increase time when downloading unnecessary checkpoints, \n",
    "# and cause conflicts when loading checkpoints.\n",
    "\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = 'gptj-6b'\n",
    "\n",
    "if model_config == 'gptj-6b':\n",
    "    model_params = {        \n",
    "        'max_context_width': 512, \n",
    "        'hidden_width': 4096, \n",
    "        'num_layers': 28, \n",
    "        'num_heads': 16,\n",
    "        \n",
    "        'tensor_parallel_degree': 4,\n",
    "        'pipeline_parallel_degree': 2,\n",
    "\n",
    "        'train_batch_size': 8,\n",
    "        'val_batch_size': 8,\n",
    "        'prescaled_batch': 1,\n",
    "    }\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for local mode\n",
    "# model_config = 'gptj-6b'\n",
    "\n",
    "# if model_config == 'gptj-6b':\n",
    "#     model_params = {        \n",
    "#         'max_context_width': 512, \n",
    "#         'hidden_width': 1024, \n",
    "#         'num_layers': 12, \n",
    "#         'num_heads': 8,\n",
    "        \n",
    "#         'tensor_parallel_degree': 4,\n",
    "#         'pipeline_parallel_degree': 2,\n",
    "\n",
    "#         'train_batch_size': 8,\n",
    "#         'val_batch_size': 8,\n",
    "#         'prescaled_batch': 1,\n",
    "#     }\n",
    "# else:\n",
    "#     raise RuntimeError(\"Unknown model config\")\n",
    "\n",
    "# for k, v in model_params.items():\n",
    "#     hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    try:\n",
    "        sm_experiment = Experiment.load(experiment_name)\n",
    "    except:\n",
    "        sm_experiment = Experiment.create(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial(experiment_name, i_type, i_cnt, pp_degree, tp_degree, batch_size):\n",
    "    create_date = strftime(\"%m%d-%H%M%s\")\n",
    "\n",
    "    i_tag = 'test'\n",
    "    if i_type == 'ml.p4d.24xlarge':\n",
    "        i_tag = 'p4d'    \n",
    "        \n",
    "    trial = \"-\".join([i_tag,str(i_cnt),f\"tp{tp_degree}\",f\"pp{pp_degree}\", f\"bs{batch_size}\"])\n",
    "       \n",
    "    sm_trial = Trial.create(trial_name=f'{experiment_name}-{trial}-{create_date}',\n",
    "                            experiment_name=experiment_name)\n",
    "\n",
    "    job_name = f'{sm_trial.trial_name}'\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [`SageMaker Estimator API`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type and Number of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import datetime\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\" # \"ml.p3.16xlarge\"\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 2\n",
    "# processes_per_host = 8\n",
    "max_run = 4*60*60      ##### 최대 학습 시간 (28일까지 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify code and output bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingjob_bucket='trainingjob-us-west-2-cyj/gpt-j' #<== 학습 후 결과를 저장하는 bucket 이름으로 변경  #<== 고객 환경이 맞게 변경\n",
    "code_location = f's3://{trainingjob_bucket}/backup_codes'\n",
    "output_path = f's3://{trainingjob_bucket}/gpt_neox_output' \n",
    "s3_log_path = f's3://{trainingjob_bucket}/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "flag = 'smmp'\n",
    "\n",
    "if flag == 'smddp':\n",
    "    distribution[\"smdistributed\"]={ \n",
    "                        \"dataparallel\": {\n",
    "                            \"enabled\": True\n",
    "                        }\n",
    "                }\n",
    "\n",
    "elif flag == 'smmp':\n",
    "    distribution['smdistributed'] = {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\":True,\n",
    "            \"parameters\": {\n",
    "                \"ddp\": True,\n",
    "                \"tensor_parallel_degree\": hyperparameters['tensor_parallel_degree'],\n",
    "                # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                # these two map to the same config\n",
    "                \"partitions\": hyperparameters['pipeline_parallel_degree'],\n",
    "                \"shard_optimizer_state\": hyperparameters['shard_optimizer_state'] > 0,\n",
    "                \"prescaled_batch\": hyperparameters['prescaled_batch'] > 0,\n",
    "                \"fp16_params\": hyperparameters['fp16'] > 0,\n",
    "                \"optimize\": hyperparameters['optimize'],\n",
    "                \"auto_partition\": True,\n",
    "                \"default_partition\": 0,                        \n",
    "                \"fp16_params\": hyperparameters['fp16'] > 0,\n",
    "                \"optimize\": hyperparameters['optimize'],\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    "    mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "    mpioptions += \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "\n",
    "    distribution[\"mpi\"]={\n",
    "        \"enabled\": True,\n",
    "        \"processes_per_host\": 8, # Pick your processes_per_host\n",
    "        \"custom_mpi_options\": mpioptions      \n",
    "    }\n",
    "else:\n",
    "    distribution[\"mpi\"]={\"enabled\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use local mode / Script mode setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    train = f'file://{Path.cwd()}/dataset/wikicorpus_en_abstract/train'\n",
    "    test = f'file://{Path.cwd()}/dataset/wikicorpus_en_abstract/test'\n",
    "    data_channels = {\"train\": train, \"test\": test}\n",
    "    checkpoint_s3_uri = None\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    train = f's3://{data_bucket}/wikicorpus_en_abstract/train'\n",
    "    test = f's3://{data_bucket}/wikicorpus_en_abstract/test'\n",
    "    if use_fsx:\n",
    "        data_channels = {\"train\": fs_train, \"test\": fs_test}\n",
    "\n",
    "        # Use the security group and subnet that was used to create the fsx filesystem\n",
    "        kwargs[\"security_group_ids\"] = [\"sg-XXXXXXXXX\"]  ## 학습인스턴스 용 보안그룹\n",
    "        kwargs[\"subnets\"] = [\"subnet-XXXXXXXXXXX\"]       ## FSX 생성 시 설정한 동일 subnet\n",
    "    else:\n",
    "        data_channels = {\"train\": train, \"test\": test}\n",
    "    checkpoint_s3_uri = f's3://{trainingjob_bucket}/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker PyTorch Estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `train_gptj_simple.py` file and the private preview documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"experiment_name : {} \\ntrain_instance_type : {} \\ntrain_instance_count : {} \\ndistribution : {}\".format(experiment_name, instance_type, instance_count, distribution))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator = HuggingFace(\n",
    "        entry_point=\"train_gptj_simple.py\",\n",
    "        source_dir=os.getcwd() + \"/code\",\n",
    "        role=role,\n",
    "        instance_type=instance_type,\n",
    "#         image=image_uri,\n",
    "        volume_size=volume_size,\n",
    "        instance_count=instance_count,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        distribution=distribution,\n",
    "        pytorch_version='1.10',\n",
    "        transformers_version='4.17',\n",
    "        py_version='py38',\n",
    "        code_location = code_location,\n",
    "        output_path=output_path,\n",
    "        disable_profiler=True,\n",
    "        debugger_hook_config=False,\n",
    "        checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "        metric_definitions=metric_definitions,\n",
    "        hyperparameters=hyperparameters,\n",
    "        max_run=max_run,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPT-J model with tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo rm -rf ./code/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_experiment(experiment_name)\n",
    "job_name = create_trial(experiment_name, instance_type, instance_count, hyperparameters['pipeline_parallel_degree'], hyperparameters['tensor_parallel_degree'], hyperparameters[\"train_batch_size\"])\n",
    "\n",
    "smp_estimator.fit(\n",
    "    inputs=data_channels, \n",
    "    job_name=job_name,\n",
    "    experiment_config={\n",
    "      'TrialName': job_name,\n",
    "      'TrialComponentDisplayName': job_name,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=smp_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of algo-1 as that is the master node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see *Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics* in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "# Deploying Trained Model for Inference\n",
    "\n",
    "In most cases the trained model can be deployed on a single device for inference, since inference has smaller memory requirements. You can use the SMP API to create a single, unified model after training. For TensorFlow, a SavedModel can be created using `smp.DistributedModel.save_model` API, and for PyTorch, `smp.save()` can be used.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker batch transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
